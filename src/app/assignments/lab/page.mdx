import { Box } from '@mui/material';
import Image from "next-image-export-optimizer";

import { CourseDataLink } from "@/components/links/CourseDataLink";
import { UnderDevelopment } from "@/components/UnderDevelopment";
import { default as ContributionStatementsReference } from "@/content/ContributionStatementsReference.mdx";
import { courseData } from "@/data/CourseData";
import { Paths } from "@/paths";
import { GlobalStyles } from '@mui/material';

import imgStoryGen from "public/images/lab/story-gen-1.png";
import imgStoryGen0 from "public/images/lab/story-gen-0.png";
import imgOllama from "public/images/lab/ollama.png";

<GlobalStyles styles={{
    'pre, code': {
        backgroundColor: '#e8e8e8',
        borderRadius: '4px',
        padding: '0.2rem',
        'font-size': '0.88rem',
        overflow: 'auto',
    }
}} />

# Technology Lab

<UnderDevelopment display={true} />

You will be creating an interactive application prototype that presents data generated or processed by an LLM in a user interface. You can choose to implement this app on web or mobile, using your preferred framework.

The goal of this technology lab is for you to get familiarized with interfacing with LLMs, integrating LLMs with a UI framework, and handling data persistence.

This document will first outline the requirements for this lab, then show examples that may be helpful for you to get started with the lab.

## Requirements
- You may complete this lab individually or in groups of two.
- Your prototype needs to have one main functionality that interfaces with LLMs, either by calling a hosted service (e.g., Gemini's API) or by calling a local model (e.g., a local LLM through ollama).
- If you prefer, you may consult AI assistants such as ChatGPT when coding this lab. You need to document how you iterated with the assistant to get a desired response, whether you adopted the LLM-generated code and why, and what modifications you made to the code (briefly document and track with Git).
- You will need to record a video demo of your prototype.

### Data Processing Requirements
- **Data pipeline**: The data presented in your prototype needs to be meaningfully transformed by at least one other method in addition to calling the LLM. For example: 
    - Pre-processing user input (e.g., analyzing user interaction logs to identify gestures)
    - Chaining LLMs together (e.g., an output from an LLM is passed to another LLM as input to support a complex task, or having an LLM that routes different tasks to appropriate downstream LLMs)
    - Post-processing data generated from the LLM (e.g., parsing a JSON object generated by an LLM to get data that can populate a list view) before presenting it in the UI
- **Data persistence**: Data relevant to that main functionality needs to be persisted after closing and reopening your prototype.

### Technical Requirements
Creating this project is going to require:
- A user interface framework
- Connecting with AI
- Storage

### Deliverables
After completing the technology lab, you will need to produce and submit:
- A version-tracked GitLab repository containing your prototype's source, including
    - A readme file that details how to build and run your prototype and external resources used
- A video demo of your prototype
- (Optional) Live demo in class


## Example 1: Creating an AI Story Generator Using Next.js
In this example ([GitLab link](https://gitlab.cs.washington.edu/cse481l-25sp/tech-lab-demo)), I created an AI story generator that accepts a user prompt describing a story they'd like to generate. The prompt, along with a system prompt, is sent to the Gemini API, which returns a generated story. The past stories are stored in browser's local storage.

You can reference this example to get a sense of how the different components may come together in your project.

<Box display="flex" flexDirection="row" alignItems="flex-start">
  <Box marginTop={-2} marginBottom={-2} marginRight={2} sx={{ mx: 'auto' }}>
    <p style={{ textAlign: 'center' }}><strong>Story Generator Prototype Web App</strong></p>
    <p>
      <Image src={imgStoryGen}
             basePath={Paths.basePath}
             height="500"
             alt="Screenshot of the Story Generator webpage, showing a list of past stories on the left. The second story is selected. On the right, there is a prompt entry box, a button to regenerate the story, and story content corresponding to the selected story."
      />
    </p>
  </Box>
</Box>

### Implementation

For this project, I used:
- **UI Framework**: Next.js with React and Tailwind CSS
- **AI Connection**: Google's Gemini 2.0 Flash
- **Storage**: Local browser storage to save previous user prompts and generated stories

### Steps I Took
1. I first followed [Next.js's guide](https://nextjs.org/docs/app/getting-started/installation) to create a new project. I used the `npx create-next-app@latest` command and accepted all default settings (i.e., TypeScript, TailWind CSS, App Router, etc.).

2. I installed the packages for Gemini (`@google/generative-ai`) and for request handling (`axios`) using `npm`.

3. I created folders `api/generateStory` under `src/app/`. Following [Gemini SDK's usage example](https://github.com/google-gemini/generative-ai-js?tab=readme-ov-file#usage-example), I added code to initialize the model and run a prompt in `route.tsx`. I added an `.env.local` file in the project directory and put in my Gemini API key obtained [here](https://aistudio.google.com/app/apikey) as `GEMINI_API_KEY=<my_gemini_api_key>`. (You will need to complete a registration process; they have a free tier which will be sufficient for this class.)

4. I asked Claude in VS Code Copilot to create code for a prompt entry box and submit button in `page.tsx` and modify the above `route.tsx` to accept a `POST` request from the page. I further refined the design a bit and added support for dark mode. ([GitLab Commit](https://gitlab.cs.washington.edu/cse481l-25sp/tech-lab-demo/-/commit/cf9b952cf1b968fee1ea0f0a1c8bad9fc02b843e))

<Box display="flex" flexDirection="row" alignItems="flex-start">
  <Box marginTop={-2} marginBottom={-2} marginRight={2} sx={{ mx: 'auto' }}>
    <p style={{ textAlign: 'center' }}><strong>Story Generator Dev Prototype: Text Box with Button</strong></p>
    <p>
      <Image src={imgStoryGen0}
             basePath={Paths.basePath}
             height="500"
             alt="Screenshot of a Story Generator development prototype webpage, showing a prompt entry box and a button to generate a story."
      />
    </p>
  </Box>
</Box>

5. After testing that I can generate and get a story back, I added local storage using `localStorage.setItem` and `localStorage.getItem` and stored the user prompt and generated as a JSON string.

6. Now my codebase is pretty cluttered with all the features in `page.tsx`. I refactored the components with the help with Copilot and put them into their respective files in `src/app/components`.

7. I asked Copilot to help me create a sidebar that displays past stories which supports adding and deleting entries. ([GitLab Commit](https://gitlab.cs.washington.edu/cse481l-25sp/tech-lab-demo/-/commit/42e251f661f8081fa3e2e8da00f34a4a1db08450))

8. I tested the prototype, identified minor stylistic and functional issues, and made modifications in code.


## Example 2: Using a Locally Hosted LLM
In another [GitLab branch](https://gitlab.cs.washington.edu/cse481l-25sp/tech-lab-demo/-/tree/ollama), I connected my story generator app to a local LLM hosted on [ollama](https://ollama.com/). You can find a list of models it supports [here](https://ollama.com/library).

1. I started the ollama server using `ollama serve`. Now I can use `ollama run <model_name>` in another terminal window to start chat with a model. I selected `gemma3` (4B) for this demo.

<Box display="flex" flexDirection="row" alignItems="flex-start">
  <Box marginTop={-2} marginBottom={-2} marginRight={2} sx={{ mx: 'auto' }}>
    <p style={{ textAlign: 'center' }}><strong>Ollama Running in Terminal</strong></p>
    <p>
      <Image src={imgOllama}
             basePath={Paths.basePath}
             height="450"
             alt="Screenshot of a ollama running in terminal with the gemma3 model. The prompt is 'Generate a story about a cat playing a violin.' The model generates a long story, starting with 'The rain in Oakhaven was a mournful, insistent thing, drumming a steady rhythm against the windows of Silas Blackwood’s cottage.'"
      />
    </p>
  </Box>
</Box>

2. In the above project, I installed `ollama` using `npm i ollama`. I refered to their [documentation](https://github.com/ollama/ollama-js) and swapped out the Gemini API for the ollama API.

3. That's it! The Story Generator should continue to work, but now it's driven by a local LLM running on your computer.


## Assessment of the Above Prototype
Here's a short reflection, or how the staff may assess the above prototype if it was turned in as an assignment:
- It meets most of the technical requirements and includes the use of a UI framework, integrating an AI, and persistent storage.
- It falls short on data processing, as the prototype only adds a short system prompt to the user input, and directly displays the LLM's output on the UI.
- Some artifects of AI code generation exist, such as code that is not very readable in `StoryHistoryItem.tsx` lines 35-37:
    ```typescript
    className={`p-3 pl-6 border-b border-gray-200 dark:border-gray-700 cursor-
    pointer hover:bg-gray-100 dark:hover:bg-gray-700 flex justify-between ${
        isSelected ? "bg-blue-50 dark:bg-blue-900/30" : ""
    }`}
    ```
Overall, this prototype will likely get a ⭐️⭐️⭐️ rating (Near Expectations) due to the above assessment.